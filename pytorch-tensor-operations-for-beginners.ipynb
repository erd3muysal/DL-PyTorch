{
 "cells": [
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {
    "id": "6j77ty7d3u04"
   },
   "source": [
    "# Deep Learning with Python + PyTorch from Zero to GANs\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "A short introduction to PyTorch and the chosen functions with short explanations.\n",
    "1. Tensors: Data Types, Cuda Feature, Numpy Arrays, Shape, Flattening, View, Ones, Zeros, Random\n",
    "2. Backward Propagation and Gradient\n",
    "3. Arithmetic and Logical Operations\n",
    "4. max() Function\n",
    "5. Bounded and Stepped Tensors"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {
    "id": "EFi_JA5A7M7z"
   },
   "source": [
    "## Environment\n",
    "In this series of articles, I will use Kaggle as an environment, however you are free to choose another platforms such as Google Colab, Binder, Amazon or your local machine. Kaggle is an invaluable free platform for AI and Data Science applications, tutorials and competitions developed and maintained by Google while a Jupyter Notebook frontend running on Googleâ€™s servers. You can get GPU/TPU support for more powerful computing. It is completely free. Kaggle comes with some preinstalled configuration, probably with tools prefered by Google. Eventhough, some of the librariers and frameworks are not preinstalled in the default configuration so if your needs are not present on Kaggle you should install them by yourself."
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {
    "id": "WuaVLQo9lJ4O"
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "Anaconda is not preinstalled in the default configuration. We should install Anaconda to install PyTorch. Follow Setup section to install prerequisties it. Follow Setup section to install prerequisties it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "WplZr-4s6-ca",
    "outputId": "9195bed8-03e8-4250-9846-b2350f859bb2"
   },
   "outputs": [],
   "source": [
    "# Anaonda installition\n",
    "#!wget https://repo.anaconda.com/archive/Anaconda3-5.2.0-Linux-x86_64.sh && bash Anaconda3-5.2.0-Linux-x86_64.sh -bfp /usr/local"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "5xMV2Uay7UwF"
   },
   "outputs": [],
   "source": [
    "# MiniConda installition\n",
    "#!wget https://repo.continuum.io/miniconda/Miniconda3-4.5.4-Linux-x86_64.sh && bash Miniconda3-4.5.4-Linux-x86_64.sh -bfp /usr/localv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "OClTOisj7ehV",
    "outputId": "ffdc790b-5092-47d4-a6d1-5a1cced63216"
   },
   "outputs": [],
   "source": [
    "# Test installition\n",
    "#!conda info --all\n",
    "#!conda list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "Z2rsMMio3xjv",
    "outputId": "c65b17b6-c1e0-4613-b920-1ec12a023a0c"
   },
   "outputs": [],
   "source": [
    "# To install PyTorch you can use following command\n",
    "#!conda install pytorch cpuonly -c pytorch -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "i68RLQdy3u05"
   },
   "outputs": [],
   "source": [
    "# Import PyTorch\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {
    "id": "WBZTn5aM3u09"
   },
   "source": [
    "### 1. Tensors: Data Types, Cuda Feature, Shape, Reshape, View, Ones, Zeros, Randn\n",
    "\n",
    "PyTorch performs all its operations using tensors. Tensor is general term for vector and matrix representation. You can represent multi dimensional matrices with tensors. Every tensor is a matrix, but vice versa is not valid.\n",
    "\n",
    "Tensors are seperated into two categories: CPU tensors and GPU tensors. We will come back this later. \n",
    "\n",
    "There are many tensor functions represented with the name of the datatypes. We can define tensors with these name-specific functions. Some of them are indicated below. Data types are not limited with the ones below. For further information, please check out the PyTorch documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "qfJt2T2-3u0-"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([6.]),\n",
       " tensor([19.2300], dtype=torch.float64),\n",
       " tensor([0], dtype=torch.int32),\n",
       " tensor([True]))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_Float = torch.FloatTensor([6.])\n",
    "t_Double = torch.DoubleTensor([19.23])\n",
    "t_Int = torch.IntTensor([0.])\n",
    "t_Bool = torch.BoolTensor([True])\n",
    "\n",
    "t_Float, t_Double, t_Int, t_Bool"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {
    "id": "Ix7VccdwBhJM"
   },
   "source": [
    "One other way to define a tensor is using `tensor` function. We can indicate data type of the tensor by simply adding the data type `dtype=X` flag. Default data type for the `tensor` function is float32. Examples of tensors can be seen below.\n",
    "\n",
    "NOTE: You can not use string data types in tensors!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "kB-DcM98BlMJ"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([6.]),\n",
       " tensor([19.2300], dtype=torch.float64),\n",
       " tensor([0], dtype=torch.int32),\n",
       " tensor([True]))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t_Float = torch.tensor([6.], dtype=torch.float32)\n",
    "t_Double = torch.tensor([19.23], dtype=torch.float64)\n",
    "t_Int = torch.tensor([0.], dtype=torch.int32)\n",
    "t_Bool = torch.tensor([True], dtype=torch.bool)\n",
    "\n",
    "t_Float, t_Double, t_Int, t_Bool"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {
    "id": "knwEFSLWKWbo"
   },
   "source": [
    "### Tensor Examples with Different Dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "HxyYwaImBfck",
    "outputId": "f4637ca2-c4a6-48cb-9e48-13c4216eb881"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(6.)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example 1 - Constant Tensor\n",
    "t0 = torch.tensor(6.)  # dtype=float32 by default\n",
    "t0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "IUwMpccd9XtC",
    "outputId": "7690c574-b363-4a32-c3ff-e7067c9bed19"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 2., 3.], dtype=torch.float64)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example 2 - Vector (1 dimensional tensor) with 3 elements\n",
    "t1 = torch.tensor([1., 2., 3.], dtype=torch.float64)\n",
    "t1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "HND_45X69cKF",
    "outputId": "8fe85b7d-976c-4c36-ef60-4e23f199cda0"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1., -2., -3., -4.],\n",
       "        [-5., -6., -7., -8.]], dtype=torch.float64)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example 3 - 2 by 4 matrix (2 dimensional tensor) with 4 element in each dimension\n",
    "t2 = torch.tensor([[-1., -2., -3., -4.],\n",
    "                   [-5., -6., -7., -8.]], dtype=torch.float64)\n",
    "t2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "uW2gfFSY9ttU",
    "outputId": "9f8db94f-1bb8-44ed-dcef-7fac826de2f9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[  0.,   2.,   4.],\n",
       "         [  6.,   8.,  10.]],\n",
       "\n",
       "        [[-10.,  -8.,  -6.],\n",
       "         [ -4.,  -2.,   0.]],\n",
       "\n",
       "        [[  0.,   2.,   4.],\n",
       "         [  6.,   8.,  10.]]], dtype=torch.float64)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example 4 - 3 dimensional tensor\n",
    "t3 = torch.tensor([[[0., 2., 4.],\n",
    "                    [6., 8., 10.]],\n",
    "                   [[-10., -8., -6.],\n",
    "                    [-4., -2., 0.]],\n",
    "                   [[0., 2., 4.],\n",
    "                    [6., 8., 10.]]], dtype = torch.float64)\n",
    "t3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "WFBpUETND3X6",
    "outputId": "e4b372e4-1b01-4ad6-9c6b-65da2716d15d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[1., 3.],\n",
       "          [1., 3.]],\n",
       "\n",
       "         [[1., 2.],\n",
       "          [1., 3.]],\n",
       "\n",
       "         [[1., 2.],\n",
       "          [1., 3.]]],\n",
       "\n",
       "\n",
       "        [[[1., 3.],\n",
       "          [1., 3.]],\n",
       "\n",
       "         [[1., 2.],\n",
       "          [1., 3.]],\n",
       "\n",
       "         [[1., 2.],\n",
       "          [1., 3.]]],\n",
       "\n",
       "\n",
       "        [[[1., 3.],\n",
       "          [1., 3.]],\n",
       "\n",
       "         [[1., 2.],\n",
       "          [1., 3.]],\n",
       "\n",
       "         [[1., 2.],\n",
       "          [1., 3.]]],\n",
       "\n",
       "\n",
       "        [[[1., 3.],\n",
       "          [1., 3.]],\n",
       "\n",
       "         [[1., 2.],\n",
       "          [1., 3.]],\n",
       "\n",
       "         [[1., 2.],\n",
       "          [1., 3.]]]], dtype=torch.float64)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example 5 - 4 dimensional tensor\n",
    "t4 = torch.tensor([[[[1., 3.], [1., 3.]], [[1., 2.], [1., 3.]], [[1., 2.], [1., 3.]]],\n",
    "                   [[[1., 3.], [1., 3.]], [[1., 2.], [1., 3.]], [[1., 2.], [1., 3.]]],\n",
    "                   [[[1., 3.], [1., 3.]], [[1., 2.], [1., 3.]], [[1., 2.], [1., 3.]]],\n",
    "                   [[[1., 3.], [1., 3.]], [[1., 2.], [1., 3.]], [[1., 2.], [1., 3.]]]], dtype = torch.float64)\n",
    "t4"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {
    "id": "HvyD44fa83UN"
   },
   "source": [
    "We mentioned GPU feature of Kaggle before. Also we talked about GPU tensors. We can store our tensors in GPU. We can use cuda function to use GPU tensors.\n",
    "\n",
    "NOTE: To be able to execete operations on GPU, you should set your accelarator as GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "1JDkw82N8aRk",
    "outputId": "5c46728b-8231-4e66-deb0-dc875e8e2272"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[1., 2.],\n",
       "          [1., 3.]],\n",
       "\n",
       "         [[1., 2.],\n",
       "          [1., 3.]],\n",
       "\n",
       "         [[1., 2.],\n",
       "          [1., 3.]]],\n",
       "\n",
       "\n",
       "        [[[1., 2.],\n",
       "          [1., 3.]],\n",
       "\n",
       "         [[1., 2.],\n",
       "          [1., 3.]],\n",
       "\n",
       "         [[1., 2.],\n",
       "          [1., 3.]]],\n",
       "\n",
       "\n",
       "        [[[1., 2.],\n",
       "          [1., 3.]],\n",
       "\n",
       "         [[1., 2.],\n",
       "          [1., 3.]],\n",
       "\n",
       "         [[1., 2.],\n",
       "          [1., 3.]]],\n",
       "\n",
       "\n",
       "        [[[1., 2.],\n",
       "          [1., 3.]],\n",
       "\n",
       "         [[1., 2.],\n",
       "          [1., 3.]],\n",
       "\n",
       "         [[1., 2.],\n",
       "          [1., 3.]]]], device='cuda:0')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example 7 - 4 dimensional GPU tensor\n",
    "t4_GPU = torch.cuda.FloatTensor([[[[1., 2.], [1., 3.]], [[1., 2.], [1., 3.]], [[1., 2.], [1., 3.]]],\n",
    "                                 [[[1., 2.], [1., 3.]], [[1., 2.], [1., 3.]], [[1., 2.], [1., 3.]]],\n",
    "                                 [[[1., 2.], [1., 3.]], [[1., 2.], [1., 3.]], [[1., 2.], [1., 3.]]],\n",
    "                                 [[[1., 2.], [1., 3.]], [[1., 2.], [1., 3.]], [[1., 2.], [1., 3.]]]])\n",
    "t4_GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {
    "id": "JAlHCjHkUGUo"
   },
   "source": [
    "Alternatively we can use device flag. To do this run the command below. It is simply assign GPU device into a variable. After assigning the GPU device into a variable, we need to set `device=X` flag with our variable in tensor definitions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "iJhfpeylGPve"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cuda0 = torch.device('cuda:0')\n",
    "cuda0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "sYVWacijF02V",
    "outputId": "976b6981-1a25-40ee-b860-f158a59f635a"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[1., 3.],\n",
       "          [1., 3.]],\n",
       "\n",
       "         [[1., 2.],\n",
       "          [1., 3.]],\n",
       "\n",
       "         [[1., 2.],\n",
       "          [1., 3.]]],\n",
       "\n",
       "\n",
       "        [[[1., 3.],\n",
       "          [1., 3.]],\n",
       "\n",
       "         [[1., 2.],\n",
       "          [1., 3.]],\n",
       "\n",
       "         [[1., 2.],\n",
       "          [1., 3.]]],\n",
       "\n",
       "\n",
       "        [[[1., 3.],\n",
       "          [1., 3.]],\n",
       "\n",
       "         [[1., 2.],\n",
       "          [1., 3.]],\n",
       "\n",
       "         [[1., 2.],\n",
       "          [1., 3.]]],\n",
       "\n",
       "\n",
       "        [[[1., 3.],\n",
       "          [1., 3.]],\n",
       "\n",
       "         [[1., 2.],\n",
       "          [1., 3.]],\n",
       "\n",
       "         [[1., 2.],\n",
       "          [1., 3.]]]], device='cuda:0', dtype=torch.float64)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example 6 - 4 dimensional GPU tensor\n",
    "t4_GPU = torch.tensor([[[[1., 3.], [1., 3.]], [[1., 2.], [1., 3.]], [[1., 2.], [1., 3.]]],\n",
    "                       [[[1., 3.], [1., 3.]], [[1., 2.], [1., 3.]], [[1., 2.], [1., 3.]]],\n",
    "                       [[[1., 3.], [1., 3.]], [[1., 2.], [1., 3.]], [[1., 2.], [1., 3.]]],\n",
    "                       [[[1., 3.], [1., 3.]], [[1., 2.], [1., 3.]], [[1., 2.], [1., 3.]]]], dtype = torch.float64, device=cuda0)\n",
    "\n",
    "t4_GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(device(type='cpu'), device(type='cuda', index=0))"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t4.device, t4_GPU.device"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "Even though PyTorch performs all operations using tensors, sometimes datasets comprise of numpy arrays, in such situations we have to transform numpy arrays to PyTorch tensors. We can transform numpy arrays to the tensors via `from_numpy()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(numpy.ndarray, torch.Tensor)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "mx = np.array([[23, 67, 43],  # Define a multi dimensional numpy array\n",
    "               [91, 88, 64], \n",
    "               [38, 35, 76], \n",
    "               [13, 43, 37], \n",
    "               [69, 56, 70]], dtype='float32')\n",
    "\n",
    "t = torch.from_numpy(mx)\n",
    "type(mx), type(t)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "Also we can transform tensors back to numpy arrays via `numpy()` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mx = t.numpy()\n",
    "type(mx)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {
    "id": "-zJSqA8L3u1D"
   },
   "source": [
    "To see the shape or size of the tensors we can use two seperate function: `shape` and `size()`. These two are executing the same task, only difference is `size()` is a callable object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "z3T2DBgb3u1E",
    "outputId": "56f3dbbc-8e70-4b7c-a084-8418b2c4ff5a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of tensor t0: torch.Size([])\n",
      "Shape of tensor t1: torch.Size([3])\n",
      "Shape of tensor t2: torch.Size([2, 4])\n",
      "Shape of tensor t3: torch.Size([3, 2, 3])\n",
      "Shape of tensor t4: torch.Size([4, 3, 2, 2])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Shape of tensor t0: {}\\n\\\n",
    "Shape of tensor t1: {}\\n\\\n",
    "Shape of tensor t2: {}\\n\\\n",
    "Shape of tensor t3: {}\\n\\\n",
    "Shape of tensor t4: {}\\n\".format(t0.shape, t1.shape, t2.shape, t3.size(), t4.size()))"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {
    "id": "YyfTo1G8CHKO"
   },
   "source": [
    "To see dimension of the tensors we can use two seperate function: `ndim` or `dim()` functions. These two are executing the same task, only difference is `dim()` is a callable object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "1kEvzK6VAsIW",
    "outputId": "8aafe913-7f7a-451f-b1a5-76191825ef75"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimension of tensor t0: 0\n",
      "Dimension of tensor t1: 1\n",
      "Dimension of tensor t2: 2\n",
      "Dimension of tensor t3: 3\n",
      "Dimension of tensor t4: 4\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"Dimension of tensor t0: {}\\n\\\n",
    "Dimension of tensor t1: {}\\n\\\n",
    "Dimension of tensor t2: {}\\n\\\n",
    "Dimension of tensor t3: {}\\n\\\n",
    "Dimension of tensor t4: {}\\n\".format(t0.ndim, t1.ndim, t2.ndim, t3.dim(), t4.dim()))"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {
    "id": "swMtgjYLD3Mp"
   },
   "source": [
    "To change and manipulate the shape of the tensor we can use several functions which are `reshape()`, `t()` and `permute(arg1, arg2, arg3)` functions. These functions are not write changes into memory, so if you want to keep shape changed, you should assign it to a new tensor or itself."
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "Changing the shape of the tensor using `reshape()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "39n3JhN4_slm",
    "outputId": "ac3ef1a1-37af-4187-cf5d-5f54f00c8f6d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[-1., -2., -3., -4.],\n",
       "         [-5., -6., -7., -8.]], dtype=torch.float64),\n",
       " torch.Size([2, 4]),\n",
       " tensor([[-1., -2., -3., -4., -5., -6., -7., -8.]], dtype=torch.float64),\n",
       " torch.Size([1, 8]))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t2_changed = t2.reshape(1, 8)\n",
    "t2, t2.shape, t2_changed, t2_changed.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {
    "id": "B3391L9yEAZ7"
   },
   "source": [
    "Taking transpose of the matrix tensor using `t()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "JAl2PkP8BATd",
    "outputId": "da2de50b-842b-4a07-d930-e93cc8795006"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[-1., -2., -3., -4.],\n",
       "         [-5., -6., -7., -8.]], dtype=torch.float64),\n",
       " torch.Size([2, 4]),\n",
       " tensor([[-1., -5.],\n",
       "         [-2., -6.],\n",
       "         [-3., -7.],\n",
       "         [-4., -8.]], dtype=torch.float64),\n",
       " torch.Size([4, 2]))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t2_changed = t2.t()\n",
    "t2, t2.shape, t2_changed, t2_changed.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "Permuting the tensor is also kind of reshaping operations with small difference; instead of indicating new shape we are indicating the order of dimensions. Please see the example usage:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[  0.,   2.,   4.],\n",
       "          [  6.,   8.,  10.]],\n",
       " \n",
       "         [[-10.,  -8.,  -6.],\n",
       "          [ -4.,  -2.,   0.]],\n",
       " \n",
       "         [[  0.,   2.,   4.],\n",
       "          [  6.,   8.,  10.]]], dtype=torch.float64),\n",
       " torch.Size([3, 2, 3]),\n",
       " tensor([[[  0.,   6.],\n",
       "          [  2.,   8.],\n",
       "          [  4.,  10.]],\n",
       " \n",
       "         [[-10.,  -4.],\n",
       "          [ -8.,  -2.],\n",
       "          [ -6.,   0.]],\n",
       " \n",
       "         [[  0.,   6.],\n",
       "          [  2.,   8.],\n",
       "          [  4.,  10.]]], dtype=torch.float64),\n",
       " torch.Size([3, 3, 2]))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Normal dimension order is 0th dimension, 1st dimension and 2nd dimension (0, 1, 2)\n",
    "t3_changed = t3.permute(0, 2, 1)  # Set dimension order as 0th dimension, 2nd dimension and 1st dimension (0, 2, 1)\n",
    "t3, t3.shape, t3_changed, t3_changed.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {
    "id": "1SXlEqkvY9ML"
   },
   "source": [
    "To flat the tensor we can use two seperate functions which are `flatten()` and `view()`. While `flatten()` flatten tensor to row vector by default, `view()` can flatten tensor to both column vector or row vector, it depends on passed argument. `reshape()` function also can be used for the same purposes. \"-1\" can be passed as an argument to `view()` and `reshape()` to flatten tensor to row vector for easy manner. These do not change the original tensor. If you want to keep tensors changed, you should assign it to a new tensor or itself."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "8MsdRgZ-YtIl",
    "outputId": "c60ca4eb-792e-4a4c-f2b7-a966bc639365"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([1., 3., 1., 3., 1., 2., 1., 3., 1., 2., 1., 3., 1., 3., 1., 3., 1., 2.,\n",
       "         1., 3., 1., 2., 1., 3., 1., 3., 1., 3., 1., 2., 1., 3., 1., 2., 1., 3.,\n",
       "         1., 3., 1., 3., 1., 2., 1., 3., 1., 2., 1., 3.], dtype=torch.float64),\n",
       " torch.Size([48]),\n",
       " tensor([1., 3., 1., 3., 1., 2., 1., 3., 1., 2., 1., 3., 1., 3., 1., 3., 1., 2.,\n",
       "         1., 3., 1., 2., 1., 3., 1., 3., 1., 3., 1., 2., 1., 3., 1., 2., 1., 3.,\n",
       "         1., 3., 1., 3., 1., 2., 1., 3., 1., 2., 1., 3.], dtype=torch.float64),\n",
       " torch.Size([48]),\n",
       " tensor([1., 3., 1., 3., 1., 2., 1., 3., 1., 2., 1., 3., 1., 3., 1., 3., 1., 2.,\n",
       "         1., 3., 1., 2., 1., 3., 1., 3., 1., 3., 1., 2., 1., 3., 1., 2., 1., 3.,\n",
       "         1., 3., 1., 3., 1., 2., 1., 3., 1., 2., 1., 3.], dtype=torch.float64),\n",
       " torch.Size([48]))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t4.flatten(), t4.flatten().shape, t4.view(-1), t4.view(-1).shape, t4.reshape(-1), t4.reshape(-1).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "Tensors can be initialized with various numeric options like zeros, ones or random numbers. To initialize tensor with zeros, we can use `zeros()`, for ones we can use `ones()`, for random numbers we can use `randn()`. All of these functions takes dimension of tensor as arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "nbAGnUiTIdyY"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0, 0, 0],\n",
       "         [0, 0, 0]], dtype=torch.int32),\n",
       " tensor([[1, 1, 1],\n",
       "         [1, 1, 1]], dtype=torch.int32),\n",
       " tensor([[ 1.2249, -1.3834, -1.0089],\n",
       "         [-0.5472,  0.1367,  0.1600]]))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zeros = torch.zeros([2, 3], dtype=torch.int32)\n",
    "ones = torch.ones([2, 3], dtype=torch.int32)\n",
    "random = torch.randn([2, 3], dtype=torch.float32)\n",
    "\n",
    "zeros, ones, random"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {
    "id": "I-xjibtZ3u1Q"
   },
   "source": [
    "## 2 - Backward and Grad\n",
    "\n",
    "Grad or gradient is a fancier term for derivative used in deep learning to represent slope of a cost function. We often use gradient calculations in deep learning algorithms. Taking derivative from end to starting point through the deep neural network called back propagation. To automize the backward process and finding gradients, PyTorch offers `backward` and `grad` functions. While `backward` function applies back propagation, `grad` function stores variable's gradients in special flags. For automatic differentiation `requires_grad` flag should be stated as \"True\" in the tensor definition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "sKLPfvGa3u1Q",
    "outputId": "ab3ed1ba-1d7c-46ba-e692-50a2414e38f8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Randomly initialized tensor: tensor([[ 1.1075, -1.7448, -0.2860, -0.7204, -0.1989],\n",
      "        [-0.0222, -0.7873,  0.5428, -0.7075, -0.3362],\n",
      "        [-0.6566,  1.0798,  0.4397,  1.4004,  0.3369]], requires_grad=True)\n",
      "\n",
      "\\Resultant tensor after pow by 3 and sum of all elements: -1.2334606647491455\n"
     ]
    }
   ],
   "source": [
    "random = torch.randn([3, 5], dtype=torch.float32, requires_grad=True)\n",
    "result = random.pow(3).sum()  # Take \"random\" tensor to the element wise power 3 and sum all of the elements\n",
    "print(\"Randomly initialized tensor: {}\\n\\n\\Resultant tensor after pow by 3 and sum of all elements: {}\".format(random, result))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "Q1CkEp2qMgR4",
    "outputId": "272628ee-d3fa-45d1-c893-e94727f213c8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[3.6799e+00, 9.1332e+00, 2.4545e-01, 1.5570e+00, 1.1867e-01],\n",
       "        [1.4759e-03, 1.8594e+00, 8.8389e-01, 1.5018e+00, 3.3911e-01],\n",
       "        [1.2934e+00, 3.4977e+00, 5.8001e-01, 5.8833e+00, 3.4041e-01]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.backward()  # Perform back propagation\n",
    "random.grad  # Print gradients of \"result\" with respect to \"random\""
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "Using `zero_()` to clearing memory of `grad` flag that keeps gradient values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "random.grad.zero_()  # Reset the gradients at the memory of tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {
    "id": "yQifCmpc3u1s"
   },
   "source": [
    "## 3 - Arithmetic and Logical Operators\n",
    "\n",
    "Various arithmetic and logical operations are available for tensors. These are not limited with the operations shown below. Also functions such as sum, square, pow, sqrt, log, mean and many more are available. Check documentation for those ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "id": "n-g1-fs73u1s"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 0.7290,  0.5909, -0.6995,  0.6383,  0.4963]),\n",
       " tensor([ 1.4270, -2.0013, -0.5771, -0.2310,  0.7540]))"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn(5)\n",
    "y = torch.randn(5)\n",
    "x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {
    "id": "33xlqxbv3u1v"
   },
   "source": [
    "Some of the arithmetic operations.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "zyj8TM2Gn4gR",
    "outputId": "ad2a090c-8e2c-4cbb-9295-e2f1bb0860bb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([ 2.1560, -1.4103, -1.2767,  0.4073,  1.2503]),\n",
       " tensor([-0.6980,  2.5922, -0.1224,  0.8693, -0.2577]),\n",
       " tensor([ 3.6452,  2.9545, -3.4977,  3.1914,  2.4814]),\n",
       " tensor([ 0.1458,  0.1182, -0.1399,  0.1277,  0.0993]))"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tAdd = torch.add(x, y)  # x + y\n",
    "tSub = torch.sub(x, y)  # x- y\n",
    "tMul = torch.mul(x, 5)  # x * 5 (element wise multiplication)\n",
    "tDiv = torch.div(x, 5)  # x / 5\n",
    "tAdd, tSub, tMul, tDiv"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "Please note that `mul()` function performs element wise multiplication. To perform matrix multiplication, we can use `@` operator.\n",
    "\n",
    "A quick linear algebra trick: to be able to succes matrix multiplication, number of columns of 1st multiplier should be equal to number of rows of the 2nd multiplier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.2588,  0.9926],\n",
       "        [ 0.5242,  0.4368],\n",
       "        [-0.1098, -1.1673]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mx_1 = torch.randn(3, 5)  # 1st multiplier\n",
    "mx_2 = torch.randn(5, 2)  # 2nd multiplier\n",
    "mul = mx_1@mx_2\n",
    "mul"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {
    "id": "hHvWr9wH7xv-"
   },
   "source": [
    "Some of the trigonometric operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "yOFdHhBv3u1w",
    "outputId": "1a1387c8-2ad7-48a3-8f6e-f94515125b63"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0.7458, 0.8304, 0.7651, 0.8031, 0.8794]),\n",
       " tensor([ 0.6662,  0.5571, -0.6439,  0.5958,  0.4762]))"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cos(x), torch.sin(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {
    "id": "VulhxbmR79cR"
   },
   "source": [
    "Let's create two boolean tensors for logical operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "dCFuEH1cseLJ"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([1, 1, 0], dtype=torch.int16), tensor([0, 1, 0], dtype=torch.int16))"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inp_1 = torch.tensor([True, True, False], dtype=torch.int16)\n",
    "inp_2 = torch.tensor([False, True, False], dtype=torch.int16)\n",
    "inp_1, inp_2"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {
    "id": "quRQ6UV_7t6P"
   },
   "source": [
    "Some of the logical operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "tLeMtjXOorWi"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([-2, -2, -1], dtype=torch.int16),\n",
       " tensor([0, 1, 0], dtype=torch.int16),\n",
       " tensor([1, 1, 0], dtype=torch.int16),\n",
       " tensor([1, 0, 0], dtype=torch.int16))"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tNot = torch.bitwise_not(inp_1, out=None)\n",
    "tAnd = torch.bitwise_and(inp_1, inp_2, out=None)\n",
    "tOr = torch.bitwise_or(inp_1, inp_2, out=None)\n",
    "tXor = torch.bitwise_xor(inp_1, inp_2, out=None)\n",
    "tNot, tAnd, tOr, tXor"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "## 4 - Max Function"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    " `max()` is widely used in deep neural network models, this makes it one of the most important function in this notebook. It is basically takes a tensor as an argument, and takes the highest number within the tensor and returns number's itself with that number's index. Follow the example for better understanding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.8799, -0.3623,  0.7082,  1.6816,  1.1184,  0.2401, -0.7956, -0.8949,\n",
       "         -0.5632,  0.2715]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tens = torch.randn(1, 10)  # Create a random tensor\n",
    "tens"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "Please note that, in our random tensor there are randomly initialized numbers and one of them is has the highest value. The number with the highest value is \"1.8813\" and the index of this number is 8. `max()` function return these two values. See below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index of the maximum value: tensor([1.6816]), maximum value: tensor([3])\n"
     ]
    }
   ],
   "source": [
    "inx, value = torch.max(tens, dim=1)\n",
    "print(\"Index of the maximum value: {}, maximum value: {}\".format(inx, value))"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {},
   "source": [
    "`max()` function is widely used with `cross_entropy()` and softmax activation function since the softmax returns probabilistic values, `max()` function is very usefull to grab the highest probability from sofmax's output."
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {
    "id": "b3vLGGlv3u15"
   },
   "source": [
    "## 5 - Bounded and Stepped Tensors\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {
    "id": "uCWsJBXa5D76"
   },
   "source": [
    "To bound all elements in an input tensor into the range of [min, max] we can use `clamp` function.\n",
    "\n",
    "Usage: *torch.clamp(input, min, max, out=None)*\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "QE_fWny13u16",
    "outputId": "aa0d1bd0-8949-4252-ab51-62f88d5827e9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.8799, 0.0000, 0.7082, 1.6816, 1.1184, 0.2401, 0.0000, 0.0000, 0.0000,\n",
       "         0.2715]])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Bounded between 0 to 10\n",
    "torch.clamp(tens, 0, 10, out=None)  # tens is randomly initialized tensor from previous cells"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {
    "id": "-lD-NBoR3u1-"
   },
   "source": [
    "\n",
    "To create a stepped vector tensor into the range of [start, end] we can use `linspace` function.\n",
    "\n",
    "Usage: *torch.linspace(start, end, steps=100, out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False)*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "fTBH6i993u1-",
    "outputId": "1b1ea9d1-a843-4038-8a69-c56e01725b95"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([  0,  25,  50,  75, 100], dtype=torch.int32)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Vector tensor started from 0 to 100 with step size of 5\n",
    "torch.linspace(0,100,steps=5,dtype=torch.int32)"
   ]
  },
  {
   "cell_type": "markdown",
   "execution_count": null,
   "metadata": {
    "id": "ts7sS1QW3u2L"
   },
   "source": [
    "## Reference Links\n",
    "* Official documentation for `torch.Tensor`: https://pytorch.org/docs/stable/tensors.html\n",
    "* PyTorch Zero to GANs Course: http://zerotogans.com/\n",
    "\n",
    "Special thanks to Jovian.ml team and freeCodeCamp.org!\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
